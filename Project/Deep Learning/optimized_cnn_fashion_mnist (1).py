# -*- coding: utf-8 -*-
"""Optimized_CNN_Fashion_Mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xpmd0PO1hTbfXNfptBCd87_63W_gFBHi
"""

!pip install -U keras-tuner

import tensorflow as tf
from tensorflow import keras 
import numpy as np
print(tf.__version__)



#download fashion mnist datset from keras 

fashion_mnist = keras.datasets.fashion_mnist

fashion_mnist

(train_images, train_labels), (test_images, test_labels)= fashion_mnist.load_data()

# we are scaling the values to reduce the computation cost.

train_images = train_images/255
test_images = test_images/255

#we are reshaping our each examle in 4-Dimension 

train_images = train_images.reshape(len(train_images), 28,28, 1)

test_images = test_images.reshape(len(test_images), 28,28,1)

# lets define a funciton for building architecture of model

def build_model(hp):
  model = keras.Sequential([
          keras.layers.Conv2D(                   #a minimal, a maximal and a default value for the Float and the Int types are given
              filters = hp.Int('conv_1_filter', min_value = 32, max_value= 128, step= 16), #hp.choice is used for creating hyperparameter search space for integer value
                                                             #optionally, a step value, i.e the minimal step between two hyperparameter values
              kernel_size = hp.Choice('conv_1_kernel', values = [3,5]), #hp.choice is used to select any value from given set of value in this case either 3 or 5
              activation = 'relu',     
              input_shape = (28,28,1)
          ),
          keras.layers.Conv2D(
              filters = hp.Int('conv_2_filter', min_value = 32, max_value= 64, step= 16),
              kernel_size = hp.Choice('conv_2_kernel', values = [3,5]),
              activation = 'relu'
          ),
          keras.layers.Flatten(),
          keras.layers.Dense(
              units = hp.Int('dense_1_units', min_value = 32, max_value=64, step = 16),
              activation = 'relu'
          ),
          keras.layers.Dense(10, 'softmax')                            
  ])

  model.compile(optimizer = keras.optimizers.Adam(hp.Choice('learning_rate', values= [1e-2, 1e-3 ])),
                loss = 'sparse_categorical_crossentropy',
                metrics = ['accuracy'])
  
  return model

from tensorflow import keras
from kerastuner.engine.hyperparameters import HyperParameters
from kerastuner.tuners import RandomSearch, Hyperband

tuner_search = RandomSearch(build_model, 
                            objective= 'val_accuracy',
                            max_trials = 5, directory = 'output', project_name = 'Mnist Fashion' )
#max_trials variable represents the number of hyperparameter combinations that will be tested by the tuner,
# execution_per_trial variable is the number of models that should be built and fit for each trial for robustness purposes. Th

#search paramter starts training all the models

tuner_search.search(train_images, train_labels, epochs = 3, validation_split = 0.1)

# finds the best model

model = tuner_search.get_best_models(1)[0]

# all hyperparamter of the best models is given

model.summary()

#best model is fit on the training set

model.fit(train_images, train_labels, epochs = 10, initial_epoch=3, validation_split = 0.15)

#predicting test accuracy

score = model.evaluate(test_images, test_labels)
print('Test loss:', score[0])

print('Test accuracy:', score[1])



"""**We are using Hyperband Instead of Randomsearch and building the model again to to improve accuracy**

#Hyperband is an optimized version of random search which uses early-stopping to speed up the hyperparameter tuning process. The main idea is to fit a large number of models for a small number of epochs and to only continue training for the models achieving the highest accuracy on the validation set. The max_epochs variable is the max number of epochs that a model can be trained for.
"""

# Hyperband is initiated and parameters are filled into it

from kerastuner.tuners import Hyperband
#HYPERBAND_MAX_EPOCHS = 10
MAX_TRIALS = 5


tuner = Hyperband(
    build_model,
    max_epochs=4,
    objective='val_accuracy',
    directory='hyperband',
    project_name='cifar10'
)

# starts building model

tuner.search(train_images, train_labels, epochs = 3, validation_split = 0.12)

model_2 = tuner.get_best_models(1)[0]

model_2.summary()

#predicting test accuracy
model_2.fit(train_images, train_labels, epochs = 10, initial_epoch=3, validation_split = 0.15)

score = model_2.evaluate(test_images, test_labels)
print('Test loss:', score[0])

print('Test accuracy:', score[1])

